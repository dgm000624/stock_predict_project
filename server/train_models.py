import yfinance as yf
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error
import xgboost as xgb
import gc
import logging
from datetime import date, timedelta, datetime
import json
import mysql.connector as mysql
import os, json, requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("train_models")

DB_CONFIG = {
    'host': '',
    'user': '',
    'password': '',
    'database': 'stock_ai_db'
}


def get_db_connection():
    try:
        conn = mysql.connect(**DB_CONFIG)
        return conn
    except mysql.Error as e:
        # DB ì—°ê²° ì˜¤ë¥˜ ì‹œ ë¡œê·¸ ì¶œë ¥ ë° None ë°˜í™˜
        logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e.msg}")
        return None

def get_and_store_stock_data(ticker, days_back=1095):
    conn = get_db_connection()
    if not conn: return None
    
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT stock_code FROM stock_info WHERE stock_code = %s", (ticker,))
        if not cursor.fetchone():
            try:
                logger.info(f"'{ticker}' ì •ë³´ê°€ stock_info í…Œì´ë¸”ì— ì—†ì–´ yfinanceì—ì„œ ì¡°íšŒ í›„ ì¶”ê°€í•©ë‹ˆë‹¤.")
                ticker_info = yf.Ticker(ticker).info
                insert_stock_info_query = "INSERT INTO stock_info (stock_code, stock_name, industry, market_type) VALUES (%s, %s, %s, %s)"
                cursor.execute(insert_stock_info_query, (ticker, ticker_info.get('longName', ticker), ticker_info.get('sector', 'N/A'), ticker_info.get('exchange', 'N/A')))
                conn.commit()
            except Exception as e:
                logger.error(f"yfinanceì—ì„œ '{ticker}' ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
                return None
        
        end_date, start_date = date.today(), date.today() - timedelta(days=days_back)
        query = "SELECT date, open, high, low, close, volume FROM daily_price WHERE stock_code = %s AND date BETWEEN %s AND %s ORDER BY date"
        cursor.execute(query, (ticker, start_date, end_date))
        data = cursor.fetchall()
        
        if len(data) > (days_back / 365) * 252 * 0.8:
            logger.info(f"'{ticker}' ë°ì´í„°ë¥¼ DBì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            df = pd.DataFrame(data, columns=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])
            df.set_index('Date', inplace=True)
            return df

        logger.info(f"'{ticker}' ë°ì´í„°ê°€ DBì— ë¶€ì¡±í•˜ì—¬ yfinanceì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥í•©ë‹ˆë‹¤.")
        data_yf = yf.download(ticker, start=start_date, end=end_date, progress=False)
        
        if data_yf.empty:
            logger.error(f"CRITICAL: yfinanceê°€ '{ticker}'ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # ê²°ì¸¡ì¹˜(NaN) í–‰ì„ ê°„ë‹¨í•˜ê²Œ ì œê±°í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.
        data_yf.dropna(inplace=True)

        logger.info(f"yfinanceì—ì„œ '{ticker}' ë°ì´í„° {len(data_yf)}ê°œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        insert_query = "INSERT INTO daily_price (stock_code, date, open, high, low, close, volume) VALUES (%s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE open=VALUES(open), high=VALUES(high), low=VALUES(low), close=VALUES(close), volume=VALUES(volume)"
        data_to_insert = [(ticker, idx.date(), float(row['Open']), float(row['High']), float(row['Low']), float(row['Close']), int(row['Volume'])) for idx, row in data_yf.iterrows()]
        
        cursor.executemany(insert_query, data_to_insert)
        conn.commit()
        return data_yf
    except Exception as e:
        logger.error(f"get_and_store_stock_data í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        return None
    finally:
        if conn and conn.is_connected():
            conn.close()

def train_and_predict_all_models(
    ticker,
    historical_days=1095,
    n_steps=60,
    test_size=0.2,
    params_by_model=None,
    run_tag=None,
    variant="base",          # â˜… variant ì¸ì ìœ ì§€
    as_of_date=None          # (ì„ íƒ) ë§ˆì§€ë§‰ ì¥ì¼ ê³ ì • ì‹œ ì‚¬ìš©
):
    
    logger.info(f"'{ticker}' ë¶„ì„ ì‹œì‘: ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬...")
    stock_data = get_and_store_stock_data(ticker, days_back=historical_days)
    if stock_data is None or stock_data.empty:
        logger.error(f"'{ticker}'ì— ëŒ€í•œ ì£¼ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    if stock_data.isnull().sum().sum() > 0:
        stock_data.fillna(method='ffill', inplace=True)
        stock_data.fillna(method='bfill', inplace=True)

    close_prices = stock_data['Close']
    df_features = pd.DataFrame({'Close': close_prices.values.flatten()}, index=close_prices.index)
    close = df_features['Close']
    lag_list = [close.shift(i).rename(f'lag_{i}') for i in range(1, n_steps + 1)]
    lag_block = pd.concat(lag_list, axis=1)
    df_features = pd.concat([df_features, lag_block], axis=1)
    df_features = df_features.copy()
    df_features = df_features.dropna()
    if df_features.empty:
        logger.error("ë°ì´í„° ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„ì´ ë„ˆë¬´ ì§§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        return None

    y_true = df_features[['Close']]
    X_features = df_features.drop('Close', axis=1)
    split_index = int(len(X_features) * (1 - test_size))
    X_train, X_test = X_features[:split_index], X_features[split_index:]
    y_train, y_test = y_true[:split_index], y_true[split_index:]

    trained_models = {}
    conn = get_db_connection()
    if not conn:
        return None

    try:
        cursor = conn.cursor()
        # ê¸°ì¡´ ê¸°ë¡ ì •ë¦¬(ì›í•˜ë©´ ìœ ì§€/ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
        cursor.execute("DELETE FROM model_comparison_log WHERE stock_code = %s", (ticker,))
        cursor.execute("DELETE FROM model_prediction_detail WHERE stock_code = %s", (ticker,))
        conn.commit()

        sklearn_models = ['polynomial', 'lasso', 'ridge', 'elasticNet', 'xgboost', 'svm']
        dl_models = ['lstm', 'gru']

        # â”€â”€ Sklearn ê³„ì—´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for name in sklearn_models:
            mparams = (params_by_model or {}).get(name, {})
            model, x_scaler, y_scaler, poly = train_sklearn_model(X_train, y_train, name, params=mparams)
            if model:
                trained_models[name] = {'model': model, 'x_scaler': x_scaler, 'y_scaler': y_scaler, 'poly': poly}
                X_full = x_scaler.transform(X_features)
                if poly: X_full = poly.transform(X_full)
                preds_scaled = model.predict(X_full)
                preds = y_scaler.inverse_transform(np.asarray(preds_scaled).reshape(-1, 1))
                mae = mean_absolute_error(y_test, preds[split_index:])
                rmse = np.sqrt(mean_squared_error(y_test, preds[split_index:]))

                log_model_results(cursor, ticker, name, mae, rmse, split_index,
                  stock_data.index[0], stock_data.index[-1],
                  mparams, n_steps=n_steps, variant=variant)
                # ğŸš¨ ìˆ˜ì •: variant="base" ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ (app.pyì˜ ì¡°íšŒ ë¡œì§ê³¼ í†µì¼)
                log_prediction_details(cursor, ticker, name, y_true.index, y_true.values.flatten(), preds.flatten(), n_steps=n_steps, variant=variant)
                last_pred   = float(preds[-1])
                last_close  = float(stock_data['Close'].iloc[-1])

                predict_class = 'UP' if last_pred >= last_close else 'DOWN'

                delta_pct = 0.0 if last_close == 0 else abs(last_pred - last_close) / last_close
                vol = stock_data['Close'].pct_change().rolling(20).std().iloc[-1]
                vol = float(vol) if np.isfinite(vol) and vol > 0 else 0.02  # ê¸°ë³¸ ë³€ë™ì„± 2%
                score = delta_pct / (vol * 2.0)
                confidence = float(np.tanh(score))
                confidence = max(0.01, min(0.99, confidence))

                upsert_prediction_result(
                conn, ticker, date.today(), name,
                last_pred, predict_class, confidence,
                variant=variant, n_steps=n_steps
                )


                nd = next_trading_day(date.today())
                insert_future_prediction(conn, ticker, name, nd, last_pred, run_tag, variant=variant)

        # â”€â”€ DL ê³„ì—´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for name in dl_models:
            dlp = (params_by_model or {}).get(name, {})
            epochs = int(dlp.get('epochs', 10))
            units = int(dlp.get('units', 50))
            batch_size = int(dlp.get('batch_size', 32))

            train_prices = close_prices[:split_index + n_steps]
            model, scaler, _, _ = train_dl_model(train_prices, n_steps, name, epochs=epochs, units=units, batch_size=batch_size)
            if model:
                trained_models[name] = {'model': model, 'scaler': scaler}
                full_scaled = scaler.transform(close_prices.values.reshape(-1, 1))
                X_seq = np.array([full_scaled[i:i + n_steps, 0] for i in range(len(full_scaled) - n_steps)])
                preds = scaler.inverse_transform(model.predict(X_seq.reshape(-1, n_steps, 1), verbose=0))

                mae = mean_absolute_error(y_true.iloc[split_index:], preds[split_index:])
                rmse = np.sqrt(mean_squared_error(y_true.iloc[split_index:], preds[split_index:]))

                dl_logged = {"epochs": epochs, "units": units, "batch_size": batch_size, "n_steps": n_steps}
                log_model_results(cursor, ticker, name, mae, rmse, split_index,
                  stock_data.index[0], stock_data.index[-1],
                  dl_logged, n_steps=n_steps, variant=variant)
                # ğŸš¨ ìˆ˜ì •: variant="base" ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ (app.pyì˜ ì¡°íšŒ ë¡œì§ê³¼ í†µì¼)
                log_prediction_details(cursor, ticker, name, y_true.index, y_true.values.flatten(), preds.flatten(), n_steps=n_steps, variant=variant)

                # ëŒ€í‘œ ì˜ˆì¸¡(ë§ˆì§€ë§‰ ê°’) â†’ ê²°ê³¼/ìµì¼ ì €ì¥
                last_pred   = float(preds[-1])
                last_close  = float(stock_data['Close'].iloc[-1])

                predict_class = 'UP' if last_pred >= last_close else 'DOWN'
                delta_pct = 0.0 if last_close == 0 else abs(last_pred - last_close) / last_close
                vol = stock_data['Close'].pct_change().rolling(20).std().iloc[-1]
                vol = float(vol) if np.isfinite(vol) and vol > 0 else 0.02
                score = delta_pct / (vol * 2.0)
                confidence = float(np.tanh(score))
                confidence = max(0.01, min(0.99, confidence))

                upsert_prediction_result(
                conn, ticker, date.today(), name,
                last_pred, predict_class, confidence,
                variant=variant, n_steps=n_steps
                )

                nd = next_trading_day(date.today())
                insert_future_prediction(conn, ticker, name, nd, last_pred, run_tag, variant=variant)                                  

        conn.commit()
        logger.info(f"'{ticker}'ì— ëŒ€í•œ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° DB ì €ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"train_and_predict_all_models í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        if conn and conn.is_connected():
            conn.rollback()
        return None
    finally:
        if conn and conn.is_connected():
            conn.close()

def log_model_results(cursor, code, name, mae, rmse, test_idx, start, end, params, n_steps, variant="base"):
    query = """
        INSERT INTO model_comparison_log
        (stock_code, model_name, train_start, train_end, mae, rmse, test_start_index, n_steps, parameters, variant)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    """
    cursor.execute(query, (code, name, start, end, mae, rmse, test_idx, n_steps, json.dumps(params), variant))

def log_prediction_details(cursor, code, name, dates, actuals, preds, n_steps, variant="base"):
    query = """
        INSERT INTO model_prediction_detail
        (stock_code, model_name, target_date, actual_price, predicted_price, n_steps, variant)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
    """
    data = []
    for dt, actual, pred in zip(dates, actuals, preds):
        if actual is not None and pred is not None and np.isfinite(actual) and np.isfinite(pred):
            data.append((code, name, dt, float(actual), float(pred), int(n_steps), variant))
    if data:
        cursor.executemany(query, data)

def train_sklearn_model(X, y, model_name, params=None):
    params = params or {}

    # ìŠ¤ì¼€ì¼ëŸ¬: ì›ì‹œ ì½”ë“œì™€ ë§ì¶”ë ¤ë©´ ì•„ë˜ ë‘ ì¤„ë¡œ êµì²´ (ì›í•˜ë©´ ìœ ì§€ ê°€ëŠ¥)
    from sklearn.preprocessing import MinMaxScaler
    x_scaler = MinMaxScaler().fit(X)     # â† ì›ì‹œì²˜ëŸ¼ í†µì¼
    y_scaler = MinMaxScaler().fit(y)

    Xs = x_scaler.transform(X)
    ys = y_scaler.transform(y)
    model, poly = None, None

    if model_name == 'polynomial':
        deg = int(params.get('degree', 2))
        poly = PolynomialFeatures(degree=deg).fit(Xs)
        # ì›ì‹œì²˜ëŸ¼ Ridge ì‚¬ìš© + alpha ì£¼ì…
        ridge_alpha = float(params.get('ridge_alpha', 1.0))
        model = Ridge(alpha=ridge_alpha).fit(poly.transform(Xs), ys)
    else:
        # ê° ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì£¼ì…
        if model_name == 'lasso':
            model = Lasso(alpha=float(params.get('alpha', 0.01)), max_iter=int(params.get('max_iter', 10000)))
        elif model_name == 'ridge':
            model = Ridge(alpha=float(params.get('alpha', 1.0)))
        elif model_name == 'elasticNet':
            model = ElasticNet(alpha=float(params.get('alpha', 0.01)),
                               l1_ratio=float(params.get('l1_ratio', 0.5)),
                               max_iter=int(params.get('max_iter', 10000)))
        elif model_name == 'xgboost':
            model = xgb.XGBRegressor(
                objective='reg:squarederror',
                learning_rate=float(params.get('learning_rate', params.get('eta', 0.01))),
                max_depth=int(params.get('max_depth', 3)),
                n_estimators=int(params.get('n_estimators', 300)),
                subsample=float(params.get('subsample', 0.7)),
                colsample_bytree=float(params.get('colsample_bytree', 0.7)),
                random_state=int(params.get('random_state', 42)),
            )
        elif model_name == 'svm':
            model = SVR(
                C=float(params.get('C', 1.0)),
                gamma=params.get('gamma', 'auto'),   # ì›ì‹œì™€ ë™ì¼
                kernel=params.get('kernel', 'rbf'),
            )
        else:
            return None, x_scaler, y_scaler, poly

        model.fit(Xs, ys.ravel())

    return model, x_scaler, y_scaler, poly

def train_dl_model(data_series, n_steps, model_name, epochs=10, units=50, batch_size=32):
    tf.keras.backend.clear_session()
    scaler = MinMaxScaler(feature_range=(0, 1)).fit(data_series.values.reshape(-1, 1))
    scaled_data = scaler.transform(data_series.values.reshape(-1, 1))

    X, y = [], []
    for i in range(len(scaled_data) - n_steps):
        X.append(scaled_data[i:(i + n_steps), 0])
        y.append(scaled_data[i + n_steps, 0])
    if not X:
        return None, None, None, None

    X_train = np.array(X).reshape(-1, n_steps, 1)
    y_train = np.array(y)

    rnn_layer = tf.keras.layers.LSTM if model_name == 'lstm' else tf.keras.layers.GRU
    model = tf.keras.Sequential([
        rnn_layer(units, input_shape=(n_steps, 1)),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
    return model, scaler, X_train, y_train


#25-10-17ì¶”ê°€(ìµì¼ ì˜ˆì¸¡ ë¹„êµ ë° íŒŒë¼ë¯¸í„°ê°’ +,- ë¹„êµ)
import os
VARIANT = os.getenv("VARIANT", "base")

def get_conn():
    return mysql.connect(**DB_CONFIG)

def next_trading_day(d):
    # í•œêµ­ íœ´ì¼/ì£¼ë§ ì²˜ë¦¬ ê°„ë‹¨í™”(ì£¼ë§ë§Œ ìŠ¤í‚µ). í•„ìš”ì‹œ íœ´ì¼ í…Œì´ë¸”ë¡œ ë³´ê°•.
    nd = d + timedelta(days=1)
    while nd.weekday() >= 5:  # 5,6 = í† /ì¼
        nd += timedelta(days=1)
    return nd

def upsert_stock_info_if_missing(conn, stock_code, stock_name=None, industry=None, market_type=None):
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM stock_info WHERE stock_code=%s", (stock_code,))
    if not cur.fetchone():
        cur.execute(
            "INSERT INTO stock_info (stock_code, stock_name, industry, market_type) VALUES (%s,%s,%s,%s)",
            (stock_code, stock_name or stock_code, industry or "N/A", market_type or "N/A"),
        )
        conn.commit()
    cur.close()

def insert_model_logs(conn, stock_code, model_name, train_start, train_end, mae, rmse, params_json, model_path=None, variant="base"):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO model_comparison_log
        (stock_code, model_name, train_start, train_end, mae, rmse, test_start_index, parameters, model_path, variant)
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
    """, (stock_code, model_name, train_start, train_end, mae, rmse, None, json.dumps(params_json), model_path, variant))
    conn.commit()
    cur.close()

def insert_prediction_detail(conn, stock_code, model_name, target_date, actual_price, predicted_price, variant="base"):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO model_prediction_detail
        (stock_code, model_name, target_date, actual_price, predicted_price, variant)
        VALUES (%s,%s,%s,%s,%s,%s)
    """, (stock_code, model_name, target_date, actual_price, predicted_price, variant))
    conn.commit()
    cur.close()

def upsert_prediction_result(conn, stock_code, target_date, model_name, predict_value, predict_class, confidence, variant="base", n_steps=None):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO prediction_result
        (stock_code, date, model_name, predict_value, predict_class, confidence, variant, n_steps)
        VALUES (%s,%s,%s,%s,%s,%s,%s,%s)
        ON DUPLICATE KEY UPDATE
          predict_value=VALUES(predict_value),
          predict_class=VALUES(predict_class),
          confidence=VALUES(confidence),
          n_steps=VALUES(n_steps)
    """, (stock_code, target_date, model_name, predict_value, predict_class, confidence, variant, n_steps))
    conn.commit()
    cur.close()

def insert_future_prediction(conn, stock_code, model_name, prediction_date, predicted_price, run_tag=None, variant="base"):
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO future_predictions
        (stock_code, model_name, prediction_date, predicted_price, variant, run_tag)
        VALUES (%s,%s,%s,%s,%s,%s)
        ON DUPLICATE KEY UPDATE
          predicted_price=VALUES(predicted_price),
          run_tag=VALUES(run_tag)
    """, (stock_code, model_name, prediction_date, predicted_price, variant, run_tag))
    conn.commit()
    cur.close()

# === ì•„ë˜ ë‘ í•¨ìˆ˜ëŠ” ë„¤ ëª¨ë¸ ë¡œì§ì— ë§ê²Œ êµ¬í˜„ë§Œ ë°”ê¾¸ë©´ ë¨ ===
def train_and_predict_one_model(stock_code, model_name, params):
    """
    ë°˜í™˜: (train_start, train_end, mae, rmse, today_pred_price, today_conf, cls_label)
    - train_start/train_end : í•™ìŠµì— ì‹¤ì œë¡œ ì‚¬ìš©ëœ ê¸°ê°„ì˜ ì‹œì‘/ë ë‚ ì§œ
    - mae/rmse              : í…ŒìŠ¤íŠ¸ì…‹ ì„±ëŠ¥
    - today_pred_price      : ê°€ì¥ ìµœê·¼ ìœˆë„ìš°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’(= 'ì˜¤ëŠ˜'ì— í•´ë‹¹)
    - today_conf            : ë¶„ë¥˜ëŠ” ìµœëŒ€ í™•ë¥ , íšŒê·€ëŠ” 0.0(í•„ìš”ì‹œ ì •ì˜)
    - cls_label             : ë¶„ë¥˜ëŠ” ì˜ˆì¸¡ ë¼ë²¨, íšŒê·€ëŠ” ì˜ˆì¸¡ê°’ vs ë§ˆì§€ë§‰ ì¢…ê°€ì˜ up/down
    """
    # ===== 1) ë°ì´í„° ë¡œë“œ & ê¸°ë³¸ ì„¸íŒ… =====
    n_steps   = int(params.get("n_steps", 60))
    test_size = float(params.get("test_size", 0.2))
    stock_data = get_and_store_stock_data(stock_code, days_back=int(params.get("historical_days", 1095)))
    if stock_data is None or stock_data.empty:
        raise RuntimeError(f"{stock_code}: í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # Close/Volume ê¸°ì¤€ìœ¼ë¡œ ë™ íŠ¹ì„± êµ¬ì„±(ë„¤ all-in-one ë¡œì§ê³¼ ë™ì¼)
    df_features = stock_data[['Close', 'Volume']].copy()
    close = df_features['Close']
    lag_list = [close.shift(i).rename(f'lag_{i}') for i in range(1, n_steps + 1)]
    lag_block = pd.concat(lag_list, axis=1)
    df_features = pd.concat([df_features, lag_block], axis=1)

    df_features = df_features.copy()

    df_features = df_features.dropna()
    if df_features.empty:
        raise RuntimeError(f"{stock_code}: ì „ì²˜ë¦¬ í›„ ìœ íš¨ í‘œë³¸ì´ ì—†ìŠµë‹ˆë‹¤. n_steps={n_steps}ê°€ ë„ˆë¬´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    y_true      = df_features[['Close']]
    X_features  = df_features.drop('Close', axis=1)
    split_index = int(len(X_features) * (1 - test_size))
    X_train, X_test = X_features[:split_index], X_features[split_index:]
    y_train, y_test = y_true[:split_index], y_true[split_index:]

    # í•™ìŠµ êµ¬ê°„ ë‚ ì§œ(ë„¤ ë¡œê·¸ í•¨ìˆ˜ì™€ ê¸°ì¤€ í†µì¼)
    train_start = pd.to_datetime(stock_data.index[0]).date()
    # split_indexëŠ” í…ŒìŠ¤íŠ¸ ì‹œì‘ì´ë¯€ë¡œ, í•™ìŠµ ë§ˆì§€ë§‰ì€ ê·¸ ì§ì „ ìƒ˜í”Œì˜ 'ì›ë³¸' ë‚ ì§œì™€ ê±°ì˜ ê°™ìŒ
    train_end   = pd.to_datetime(stock_data.index[max(0, split_index - 1)]).date()

    # ===== 2) ëª¨ë¸ í•™ìŠµ & ì „ì²´ êµ¬ê°„ ì˜ˆì¸¡ =====
    sklearn_models = {'polynomial', 'lasso', 'ridge', 'elasticNet', 'xgboost', 'svm'}
    dl_models      = {'lstm', 'gru'}

    if model_name in sklearn_models:
        mparams = params.copy()
        model, x_scaler, y_scaler, poly = train_sklearn_model(X_train, y_train, model_name, params=mparams)
        if model is None:
            raise RuntimeError(f"{stock_code}/{model_name}: ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")

        X_full = x_scaler.transform(X_features)
        if poly:
            X_full = poly.transform(X_full)
        preds_scaled = model.predict(X_full)
        preds = y_scaler.inverse_transform(np.asarray(preds_scaled).reshape(-1, 1)).ravel()

    elif model_name in dl_models:
        dlp       = params.copy()
        epochs    = int(dlp.get('epochs', 10))
        units     = int(dlp.get('units', 50))
        batch_sz  = int(dlp.get('batch_size', 32))

        close_prices = stock_data['Close']
        # ë„¤ ê¸°ì¡´ ë¡œì§ì²˜ëŸ¼: í•™ìŠµì€ (í•™ìŠµì¢…ë£Œì§€ì  + n_steps)ê¹Œì§€ë§Œ ì‚¬ìš©
        train_prices = close_prices[:split_index + n_steps]
        model, scaler, _, _ = train_dl_model(train_prices, n_steps, model_name,
                                             epochs=epochs, units=units, batch_size=batch_sz)
        if model is None:
            raise RuntimeError(f"{stock_code}/{model_name}: DL ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")

        full_scaled = scaler.transform(close_prices.values.reshape(-1, 1))
        X_seq = np.array([full_scaled[i:i + n_steps, 0] for i in range(len(full_scaled) - n_steps)])
        # ì „ì²´ êµ¬ê°„ ì˜ˆì¸¡(ìŠ¤ì¼€ì¼ ì—­ë³€í™˜)
        preds = scaler.inverse_transform(model.predict(X_seq.reshape(-1, n_steps, 1), verbose=0)).ravel()
        # íŠ¹ì„± ë™ ë•Œë¬¸ì— df_features ê¸°ì¤€ê³¼ ê¸¸ì´ê°€ ë§ë„ë¡ íŒ¨ë”©
        # df_featuresëŠ” n_steps ì´í›„ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ predsì˜ ê¸¸ì´ì™€ ì •ë ¬ì´ ë™ì¼
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ëª…: {model_name}")

    # ===== 3) ì„±ëŠ¥/ì˜¤ëŠ˜ ì˜ˆì¸¡/ë¼ë²¨ =====
    # df_featuresëŠ” n_steps ì´í›„ ì‹œì ë¶€í„°, predsë„ ê°™ì€ ê¸°ì¤€ì´ë¯€ë¡œ ë°”ë¡œ split ì‚¬ìš© ê°€ëŠ¥
    mae  = float(mean_absolute_error(y_test.values.ravel(), preds[split_index:]))
    rmse = float(np.sqrt(mean_squared_error(y_test.values.ravel(), preds[split_index:])))

    today_pred_price = float(preds[-1])

    # ë¶„ë¥˜ë©´ í™•ë¥ , íšŒê·€ë©´ 0.0 + up/down
    today_conf = 0.0
    cls_label  = "up" if today_pred_price >= float(stock_data['Close'].iloc[-1]) else "down"

    return train_start, train_end, mae, rmse, today_pred_price, today_conf, cls_label

def run_for_ticker_list(tickers, models, run_tag=None, variant="base"):
    """
    tickers: ["005930.KS", ...]
    models:  {"gru": {...}, "lstm": {...}, "xgboost": {...}, ...}
    variant: "minus" | "base" | "plus"
    """
    ns = models.get("gru", {}).get("n_steps") or models.get("lstm", {}).get("n_steps") or 60

    for tk in tickers:
        print(f"[RUN] {tk} (models={list(models.keys())}, variant={variant}, tag={run_tag})")
        try:
            _ = train_and_predict_all_models(
                ticker=tk,
                historical_days=1095,
                n_steps=ns,
                test_size=0.2,
                params_by_model=models,
                run_tag=run_tag,
                variant=variant,     
            )
            print(f"[DONE] {tk} variant={variant}")

            inserted = compute_and_insert_alerts_direct(
                conn=get_db_connection(),       # ë„¤ê°€ ì“°ëŠ” DB ì»¤ë„¥í„°
                target_date=None,               # Noneì´ë©´ ìµœì‹  ì˜ˆì¸¡ì¼
                direction=os.getenv("ALERTS_DIRECTION", "both"),
                require_all=os.getenv("ALERTS_REQUIRE_ALL", "true").lower() == "true",
                min_confidence=(float(os.getenv("ALERTS_MIN_CONF", "0.7")) if os.getenv("ALERTS_MIN_CONF") else None),
                min_models=(int(os.getenv("ALERTS_MIN_MODELS")) if os.getenv("ALERTS_MIN_MODELS") else None),
                alert_prefix=os.getenv("ALERTS_PREFIX", "consensus")
            )
            logger.info(f"alerts.direct inserted={inserted}")

        except Exception as e:
            print(f"[ERROR] {tk} variant={variant}: {e!r}")

# === ì¶”ê°€: ì•Œë¦¼ ì§ì ‘ ìƒì„± í•¨ìˆ˜ ===
def compute_and_insert_alerts_direct(conn, target_date=None,
                                     direction="both",
                                     require_all=True,
                                     min_confidence=None,
                                     min_models=None,
                                     alert_prefix="consensus"):
    """/api/alerts/run ëŒ€ì‹  DBì— ì§ì ‘ í•©ì˜ ì•Œë¦¼ì„ ì ì¬í•œë‹¤."""
    if conn is None:
        logger.warning("ì•Œë¦¼ ìƒì„± ì‹¤íŒ¨: DB ì»¤ë„¥ì…˜ ì—†ìŒ")
        return 0

    with conn.cursor() as cur:
        # ë‚ ì§œ ë¯¸ì§€ì •ì´ë©´ prediction_result ìµœì‹ ì¼
        if target_date is None:
            cur.execute("SELECT MAX(date) AS d FROM prediction_result")
            row = cur.fetchone()
            target_date = row[0]
            if not target_date:
                logger.warning("prediction_resultê°€ ë¹„ì–´ìˆì–´ ì•Œë¦¼ ìƒì„± ìŠ¤í‚µ")
                return 0

        # confidence ì¡°ê±´
        conf_sql = "AND (confidence IS NULL OR confidence >= %s)" if (min_confidence is not None) else ""
        conf_param = [float(min_confidence)] if (min_confidence is not None) else []

        # (stock_code, date)ë³„ up/down ì¹´ìš´íŠ¸ ì§‘ê³„
        group_sql = f"""
            SELECT
                stock_code,
                date,
                COUNT(*) AS n_models,
                SUM(CASE WHEN UPPER(predict_class) IN ('UP','RISE','BUY','BULL') {conf_sql} THEN 1 ELSE 0 END) AS n_up,
                SUM(CASE WHEN UPPER(predict_class) IN ('DOWN','SELL','BEAR') {conf_sql} THEN 1 ELSE 0 END)      AS n_down
            FROM prediction_result
            WHERE date = %s
            GROUP BY stock_code, date
        """

        # ì¡°ê±´ì‹
        up_cond_sql = "g.n_up = g.n_models" if require_all else "g.n_up >= COALESCE(%s, CEIL(g.n_models/2))"
        dn_cond_sql = "g.n_down = g.n_models" if require_all else "g.n_down >= COALESCE(%s, CEIL(g.n_models/2))"

        inserted_total = 0

        def insert_by_direction(is_up: bool):
            nonlocal inserted_total
            alert_type   = f"{alert_prefix}_{'up' if is_up else 'down'}"
            dir_label    = "ìƒìŠ¹" if is_up else "í•˜ë½"
            consensus_tag = " í•©ì˜" if alert_prefix == "consensus" else ""
            conf_tag     = f" (confâ‰¥{min_confidence})" if min_confidence is not None else ""
            if not require_all:
                models_tag = f" (â‰¥{int(min_models)} models)" if (min_models is not None) else " (â‰¥ê³¼ë°˜)"
            else:
                models_tag = ""
            message_prefix = f"ëª¨ë¸{consensus_tag} {dir_label}{conf_tag}{models_tag} | ì°¸ì—¬ëª¨ë¸: "

            cond_sql = up_cond_sql if is_up else dn_cond_sql

            sql = f"""
                INSERT INTO user_alerts (stock_code, date, alert_type, message)
                SELECT
                  pr.stock_code,
                  pr.date,
                  %s AS alert_type,
                  CONCAT(%s, GROUP_CONCAT(DISTINCT pr.model_name ORDER BY pr.model_name SEPARATOR ', ')) AS message
                FROM prediction_result pr
                JOIN (
                    {group_sql}
                ) g
                  ON g.stock_code = pr.stock_code AND g.date = pr.date
                LEFT JOIN user_alerts ua
                  ON ua.stock_code = pr.stock_code
                 AND ua.date = pr.date
                 AND ua.alert_type = %s
                WHERE pr.date = %s
                  AND {cond_sql}
                  AND ua.id IS NULL
                GROUP BY pr.stock_code, pr.date
            """
            params = [
                alert_type,
                message_prefix,
                # group_sql íŒŒë¼ë¯¸í„°
            ] + conf_param + conf_param + [target_date] + [
                # LEFT JOIN alert_type, pr.date
                alert_type, target_date
            ]
            if not require_all:
                params += [min_models]

            cur.execute(sql, params)
            inserted_total += cur.rowcount

        if direction in ("up", "both"):
            insert_by_direction(True)
        if direction in ("down", "both"):
            insert_by_direction(False)

        conn.commit()
        return inserted_total




ALERTS_BASE_URL = os.getenv("ALERTS_BASE_URL", "http://localhost:5000")  # Cloud Run URLë¡œ êµì²´ ê°€ëŠ¥
ALERTS_ENDPOINT = f"{ALERTS_BASE_URL.rstrip('/')}/api/alerts/run"



def trigger_alerts_run(direction="up", require_all=True, min_confidence=None, min_models=None, alert_prefix="consensus"):
    """í•™ìŠµ ì™„ë£Œ í›„ ì•Œë¦¼ ì¬ê³„ì‚°ì„ íŠ¸ë¦¬ê±°í•œë‹¤."""
    payload = {
        "direction": direction,          # "up" | "down" | "both"
        "require_all": require_all,      # ì „(å…¨)ëª¨ë¸ ì¼ì¹˜
        "min_confidence": min_confidence,# e.g., 0.7 (ì—†ìœ¼ë©´ None)
        "min_models": min_models,        # require_all=Falseì¼ ë•Œë§Œ ì‚¬ìš©
        "alert_prefix": alert_prefix     # "consensus" | "model"
        # "date"ëŠ” ìƒëµ â†’ ë°±ì—”ë“œê°€ MAX(date) ì‚¬ìš©
    }
    try:
        r = requests.post(ALERTS_ENDPOINT, json=payload, timeout=15)
        r.raise_for_status()
        return True, r.json()
    except Exception as e:
        logger.warning(f"/api/alerts/run í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return False, {"error": str(e)}

