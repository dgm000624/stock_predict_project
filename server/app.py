import yfinance as yf
import numpy as np
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import kss
import mysql.connector
import logging
import traceback
import pytz
import os
import hashlib
from collections import OrderedDict
from flask import Flask, request, jsonify, render_template
from flask_socketio import SocketIO
from datetime import datetime, timedelta
# BERT ë¶„ì„ì„ ìœ„í•œ transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline
# LLM ë¶„ì„ì„ ìœ„í•œ Gemini API
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import json # JSON íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€
import time
from db import get_conn
from threading import Thread
from train_models import train_and_predict_all_models # train_models.pyëŠ” ê°™ì€ ê²½ë¡œì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

# ==============================================================================
# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
# ==============================================================================

app = Flask(__name__)
socketio = SocketIO(app, async_mode='eventlet', ping_timeout=60)
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)

# DB_CONFIG: ì‚¬ìš©ìê°€ ì œê³µí•œ ì„±ê³µí–ˆë˜ DB ì„¤ì • ì‚¬ìš©
DB_CONFIG = {
    'host': '', 'user': '', 'password': '', 'database': 'stock_ai_db'
}

# ë‰´ìŠ¤ ë¶„ì„ ê´€ë ¨ ì „ì—­ ì„¤ì •
GLOBAL_NEWS_CACHE = {} 
NEWS_API_KEY = "" #Newsapi.org API 
HUGGINGFACE_TOKEN = "" 
GEMINI_API_KEY = ""

GLOBAL_MODELS = {}
GLOBAL_GEMINI_MODEL = None # LLM ëª¨ë¸ ì „ì—­ ë³€ìˆ˜
SCORE_THRESHOLD = 0.6
MODEL_NAMES = [
    "snunlp/KR-FinBert-SC",
    "DataWizardd/finbert-sentiment-ko"
]


def initialize_gemini():
    """Gemini API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global GLOBAL_GEMINI_MODEL, GEMINI_API_KEY
    
    if GEMINI_API_KEY == "YOUR_GEMINI_API_KEY_DEFAULT" or not GEMINI_API_KEY:
        print("ğŸš¨ ê²½ê³ : GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ëª¨ë“œê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
        
    try:
        # í…ŒìŠ¤íŠ¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ configure í˜¸ì¶œ
        genai.configure(api_key=GEMINI_API_KEY)
        # ëª¨ë¸ë§Œ ìƒì„±
        GLOBAL_GEMINI_MODEL = genai.GenerativeModel('gemini-2.5-flash') 
        print("âœ… Gemini API ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸš¨ Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        GLOBAL_GEMINI_MODEL = None

def load_models():
    """BERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    classifiers = {}
    try:
        tokenizer1 = AutoTokenizer.from_pretrained(MODEL_NAMES[0], token=HUGGINGFACE_TOKEN)
        model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAMES[0], token=HUGGINGFACE_TOKEN)
        classifiers["classifier1"] = TextClassificationPipeline(model=model1, tokenizer=tokenizer1)
    except Exception as e:
        print(f"ğŸš¨ Model 1 ë¡œë“œ ì‹¤íŒ¨: {e}")
    try:
        tokenizer2 = AutoTokenizer.from_pretrained(MODEL_NAMES[1], token=HUGGINGFACE_TOKEN)
        model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_NAMES[1], token=HUGGINGFACE_TOKEN)
        classifiers["classifier2"] = TextClassificationPipeline(model=model2, tokenizer=tokenizer2)
    except Exception as e:
        print(f"ğŸš¨ Model 2 ë¡œë“œ ì‹¤íŒ¨: {e}")
    global GLOBAL_MODELS
    GLOBAL_MODELS = classifiers
    return classifiers

# ==============================================================================
# 2. DB ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (AI ì˜ˆì¸¡ ë³µêµ¬ ë° ë‰´ìŠ¤ ê³µí†µ)
# ==============================================================================

def normalize_key(title: str) -> str:
    """ì œëª©ì„ ì•ˆì •ì ìœ¼ë¡œ ìºì‹±í•˜ê¸° ìœ„í•œ í•´ì‹œ í‚¤ ìƒì„±"""
    if not title: return ""
    normalized = re.sub(r'\s+', ' ', title.strip().lower())
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()

def get_db_connection():
    try: return mysql.connector.connect(**DB_CONFIG)
    except mysql.connector.Error as e:
        print(f"DB ì—°ê²° ì˜¤ë¥˜: {e}"); return None

def get_historical_data_from_db(ticker):
    print(f"--- DBì—ì„œ AI ì˜ˆì¸¡ ê¸°ë¡ ì¡°íšŒ: {ticker} ---")
    conn = get_db_connection()
    if not conn: return {}
    try:
        cursor = conn.cursor(dictionary=True)
        query = "SELECT model_name, target_date, actual_price, predicted_price FROM model_prediction_detail WHERE stock_code = %s ORDER BY target_date"
        cursor.execute(query, (ticker,))
        results = cursor.fetchall()
        if not results: return {}

        data_by_date = OrderedDict()
        model_names = sorted(list(set(r['model_name'] for r in results)))
        
        for row in results:
            if any(v is None for v in [row['target_date'], row['actual_price']]): continue
            date_str = row['target_date'].strftime('%Y-%m-%d')
            if date_str not in data_by_date:
                data_by_date[date_str] = {'actual': float(row['actual_price']), 'preds': {m: None for m in model_names}}
            if row['predicted_price'] is not None:
                data_by_date[date_str]['preds'][row['model_name']] = float(row['predicted_price'])

        dates = list(data_by_date.keys())
        actuals = [d['actual'] for d in data_by_date.values()]
        predictions = {model: [data_by_date[d]['preds'].get(model) for d in dates] for model in model_names}

        cursor.execute("SELECT test_start_index FROM model_comparison_log WHERE stock_code = %s LIMIT 1", (ticker,))
        log_result = cursor.fetchone()
        
        return {
            'dates': dates, 'actuals': actuals, 'predictions': predictions,
            'test_start_index': log_result.get('test_start_index', 0) if log_result else 0
        }
    except Exception as e:
        print(f"DB ì¡°íšŒ ì¤‘ ì˜ˆì™¸: {e}"); return {}
    finally:
        if conn and conn.is_connected(): conn.close()

# get_stock_data_from_yfinance: (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
def get_stock_data_from_yfinance(ticker, period="3y"):
    print(f"--- yfinance ë‹¨ìˆœ ë°ì´í„° ì¡°íšŒ: {ticker} (ê¸°ê°„: {period}) ---")
    try:
        interval = '1m' if period == '1d' else '1d'
        data_yf = yf.download(ticker, period=period, interval=interval, progress=False, auto_adjust=True, group_by='ticker')
        if data_yf.empty: return None

        if isinstance(data_yf.columns, pd.MultiIndex):
            data_yf.columns = data_yf.columns.droplevel(0)

        if 'Close' not in data_yf.columns:
            if len(data_yf.columns) >= 4:
                data_yf.rename(columns={data_yf.columns[3]: 'Close'}, inplace=True)
            else: return None

        if period == '1d':
            dates = data_yf.index.strftime('%H:%M:%S').tolist()
        else:
            dates = data_yf.index.strftime('%Y-%m-%d').tolist()

        prices = data_yf['Close'].tolist()
        
        latest_price = prices[-1] if prices else 0
        latest_time = dates[-1] if dates else "N/A"

        return {'dates': dates, 'prices': prices, 'latest_price': latest_price, 'latest_time': latest_time}
    except Exception:
        traceback.print_exc(); return None

# get_and_cache_stock_names: (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
def get_and_cache_stock_names(tickers):
    names = {}
    conn = get_db_connection()
    if not conn:
        for ticker in tickers:
            try: names[ticker] = yf.Ticker(ticker).info.get('longName', ticker)
            except: names[ticker] = ticker
        return names

    try:
        cursor = conn.cursor(dictionary=True)
        format_strings = ','.join(['%s'] * len(tickers))
        cursor.execute(f"SELECT stock_code, stock_name FROM stock_info WHERE stock_code IN ({format_strings})", tuple(tickers))
        for row in cursor.fetchall(): names[row['stock_code']] = row['stock_name']
        
        missing_tickers = [t for t in tickers if t not in names]
        if missing_tickers:
            print(f"DBì— ì—†ëŠ” ì¢…ëª© ì •ë³´ ì¡°íšŒ: {missing_tickers}")
            for ticker in missing_tickers:
                try:
                    info = yf.Ticker(ticker).info
                    name = info.get('longName', ticker)
                    names[ticker] = name
                    insert_query = "INSERT INTO stock_info (stock_code, stock_name, industry, market_type) VALUES (%s, %s, %s, %s) ON DUPLICATE KEY UPDATE stock_name=VALUES(stock_name)"
                    cursor.execute(insert_query, (ticker, name, info.get('sector', 'N/A'), info.get('exchange', 'N/A')))
                except Exception as e:
                    print(f"yfinanceì—ì„œ {ticker} ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    names[ticker] = ticker
            conn.commit()
    except Exception as e:
        print(f"ì¢…ëª© ì´ë¦„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        if conn.is_connected(): conn.close()
    return names

def initialize_stock_data():
    print(f"[{datetime.now()}] ğŸš€ ì´ˆê¸° ì¢…ëª© ì •ë³´ ìºì‹± ì‹œì‘...")
    target_tickers = get_all_tickers_from_db()
    default_tickers = ['005930.KS', 'AAPL', 'TSLA']
    target_tickers.extend(t for t in default_tickers if t not in target_tickers)
    if not target_tickers:
        print("ê²½ê³ : ì´ˆê¸°í™”í•  ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    names = get_and_cache_stock_names(target_tickers)
    print(f"âœ… ì´ˆê¸° ì¢…ëª© ì •ë³´ ìºì‹± ì™„ë£Œ. ë¡œë“œëœ ì¢…ëª© ìˆ˜: {len(names)}")

def get_all_tickers_from_db():
    tickers = []
    conn = get_db_connection()
    if not conn: return []
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT stock_code FROM stock_info")
        tickers = [row[0] for row in cursor.fetchall()]
    except Exception:
        pass
    finally:
        if conn.is_connected(): conn.close()
    return tickers

# ==============================================================================
# 3. ë‰´ìŠ¤ ë¶„ì„ ë° ìŠ¤í¬ë˜í•‘ (BERT & LLM ê³µí†µ)
# ==============================================================================

def fetch_korean_news(query, page_size=20):
    url = "https://newsapi.org/v2/everything"
    # ê¸°ê°„ ì„¤ì •: í˜„ì¬ëŠ” ìµœê·¼ 7ì¼(1ì£¼)
    from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    params = {"q": query, "language": "ko", "pageSize": page_size, "sortBy": "publishedAt", "apiKey": NEWS_API_KEY, "from": from_date}
    articles = []
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        for article in data.get("articles", []):
            articles.append({
                "title": article.get("title", ""),
                "content": article.get("content", "") or article.get("description", ""),
                "source": article.get("source", {}).get("name", "Unknown"),
                "url": article.get("url", ""),
                "publishedAt": article.get("publishedAt", "")
            })
        return articles
    except Exception as e:
        print(f"ë‰´ìŠ¤ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []

def fetch_full_article(url):
    """ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        r = requests.get(url, timeout=5)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = ' '.join(p.get_text() for p in paragraphs)
        return text
    except Exception:
        return ""

def is_valid_sentence(s):
    return re.search(r"[ê°€-í£]", s)

# ==============================================================================
# 4. BERT ë¶„ì„ í•¨ìˆ˜
# ==============================================================================

def map_label(result):
    label = result["label"].upper()
    score = result.get("score",0)
    if "LABEL_2" in label or "POSITIVE" in label: mapped_label = "POSITIVE"
    elif "LABEL_0" in label or "NEGATIVE" in label: mapped_label = "NEGATIVE"
    else: mapped_label = "NEUTRAL"
    return mapped_label if score >= SCORE_THRESHOLD else "NEUTRAL"

def analyze_sentiment(sentence, models):
    if not models:
        return {"final_label":"NEUTRAL","model1_label":"N/A","model2_label":"N/A","model1_score":0.0,"model2_score":0.0}
    data = {"model1_label":"NEUTRAL","model1_score":0.0,"model2_label":"NEUTRAL","model2_score":0.0,"final_label":"NEUTRAL"}
    if "classifier1" in models:
        r1 = models["classifier1"](sentence)[0]
        data["model1_label"]=map_label(r1); data["model1_score"]=r1.get("score",0.0)
    if "classifier2" in models:
        r2 = models["classifier2"](sentence)[0]
        data["model2_label"]=map_label(r2); data["model2_score"]=r2.get("score",0.0)
    if data["model1_label"]!="NEUTRAL" and data["model1_label"]==data["model2_label"]: data["final_label"]=data["model1_label"]
    elif "classifier1" in models and "classifier2" not in models: data["final_label"]=data["model1_label"]
    elif "classifier2" in models and "classifier1" not in models: data["final_label"]=data["model2_label"]
    return data


# ==============================================================================
# 5. LLM (Gemini) ë¶„ì„ í•¨ìˆ˜
# ==============================================================================

LLM_OUTPUT_SCHEMA = {
    "type": "object",
    "properties": {
        "final_label": {"type": "string", "description": "ê¸°ì‚¬ì˜ ì¢…í•©ì ì¸ ê°ì„± ('POSITIVE', 'NEGATIVE', 'NEUTRAL' ì¤‘ í•˜ë‚˜)"},
        "summary": {"type": "string", "description": "ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"},
        "key_sentiment_points": {"type": "array", "description": "ê°ì„±ì„ íŒë‹¨í•œ ì£¼ìš” ê·¼ê±° 2~3ê°€ì§€", "items": {"type": "string"}}
    },
    "required": ["final_label", "summary", "key_sentiment_points"]
}

# app.py (analyze_sentiment_with_llm í•¨ìˆ˜ ë‚´ë¶€)

def analyze_sentiment_with_llm(ticker, name, full_text):
    global GLOBAL_GEMINI_MODEL
    if GLOBAL_GEMINI_MODEL is None:
        return {'status': 'error', 'message': 'Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.'}

    if len(full_text) < 100:
        return {'status': 'error', 'message': 'ê¸°ì‚¬ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ (100ì ë¯¸ë§Œ) LLM ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}

    # â˜… 1. í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ JSON ì‘ë‹µ í˜•ì‹ì„ ê°•ì œí•©ë‹ˆë‹¤.
    prompt = f"""
    ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¢…ëª© {name} ({ticker})ì— ëŒ€í•œ ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
    
    1. ì´ ê¸°ì‚¬ê°€ {ticker}ì˜ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì˜ **ì¢…í•©ì ì¸ ê°ì„±**ì„ 'POSITIVE', 'NEGATIVE', 'NEUTRAL' ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•˜ì„¸ìš”.
    2. ê¸°ì‚¬ì˜ **í•µì‹¬ ë‚´ìš©**ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
    3. í•´ë‹¹ ê°ì„±ì„ íŒë‹¨í•œ **ì£¼ìš” ê·¼ê±°** 2~3ê°€ì§€ë¥¼ ì°¾ìœ¼ì„¸ìš”.

    ë¶„ì„ ê²°ê³¼ëŠ” **ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ ë¬¸ìì—´**ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:
    {{
        "final_label": "POSITIVE ë˜ëŠ” NEGATIVE ë˜ëŠ” NEUTRAL",
        "summary": "1~2ë¬¸ì¥ ìš”ì•½",
        "key_sentiment_points": ["ê·¼ê±° 1", "ê·¼ê±° 2", "ê·¼ê±° 3"]
    }}

    --- ê¸°ì‚¬ ì „ë¬¸ ---
    {full_text}
    """
    
    try:
        # â˜… 2. API í˜¸ì¶œ ì‹œ ì¸ìë¥¼ ìµœì†Œí™”í•˜ê³  safety_settingsë¥¼ ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤.
        # êµ¬ ë²„ì „ì€ config=ë‚˜ response_mime_typeì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        response = GLOBAL_GEMINI_MODEL.generate_content(
            prompt,
            safety_settings=[ # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
            ]
        )
        
        # â˜… 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ êµ¬ë¬¸ ì œê±°
        raw_text = response.text.strip()
        
        # ```json\nìœ¼ë¡œ ì‹œì‘í•˜ê³  ```ë¡œ ëë‚˜ëŠ” ê²½ìš°, í•´ë‹¹ êµ¬ë¬¸ì„ ì œê±°í•©ë‹ˆë‹¤.
        if raw_text.startswith("```json"):
            # ì²« ì¤„ì˜ ```json\n ì œê±°
            raw_text = raw_text.lstrip("```json\n")
        if raw_text.endswith("```"):
            # ë§ˆì§€ë§‰ ì¤„ì˜ ``` ì œê±°
            raw_text = raw_text.rstrip("```")
            
        # 2. ìˆœìˆ˜í•œ JSON ë¬¸ìì—´ì„ íŒŒì‹±
        llm_result = json.loads(raw_text.strip())
        llm_result['status'] = 'success'
        return llm_result

    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëª¨ë¸ì´ í˜•ì‹ì„ ì§€í‚¤ì§€ ì•Šì€ ê²ƒì¼ ìˆ˜ ìˆìŒ
        print(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"Gemini Raw Response Text: {response.text[:200]}...")
        return {'status': 'error', 'message': f'Gemini API ë¶„ì„ ì˜¤ë¥˜: {e}. (ì‘ë‹µ í…ìŠ¤íŠ¸ í™•ì¸ í•„ìš”)'}

# ==============================================================================
# 6. Flask ë¼ìš°íŠ¸ (AI ì˜ˆì¸¡ ë³µêµ¬ ë° ë‰´ìŠ¤ ë¶„ì„ ë¶„ê¸°)
# ==============================================================================
@app.route('/get_all_industries', methods=['GET'])
def get_all_industries():
    conn = get_db_connection()
    if not conn:
        return jsonify({'status': 'error', 'message': 'DB connection failed'}), 500
    try:
        cursor = conn.cursor(dictionary=True)
        # 'N/A'ê°€ ì•„ë‹ˆê±°ë‚˜ ë¹„ì–´ìˆì§€ ì•Šì€ ìœ íš¨í•œ ì‚°ì—… ëª©ë¡ë§Œ ì¡°íšŒ
        cursor.execute("SELECT DISTINCT industry FROM stock_info WHERE industry IS NOT NULL AND industry != 'N/A' AND industry != '' ORDER BY industry")
        industries = [row['industry'] for row in cursor.fetchall()]
        return jsonify({'status': 'success', 'industries': industries})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500
    finally:
        if conn and conn.is_connected(): conn.close()

# [ì¶”ê°€] ì‚¬ìš©ìê°€ ì„ íƒí•œ ì‚°ì—…ì˜ ì£¼ì‹ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” API
@app.route('/get_stocks_by_industry', methods=['POST'])
def get_stocks_by_industry():
    data = request.json
    industry = data.get('industry')
    if not industry:
        return jsonify({'status': 'error', 'message': 'Industry required'}), 400
    
    conn = get_db_connection()
    if not conn:
        return jsonify({'status': 'error', 'message': 'DB connection failed'}), 500
    try:
        cursor = conn.cursor(dictionary=True)
        # í•´ë‹¹ ì‚°ì—…ì˜ ì£¼ì‹ì„ 10ê°œê¹Œì§€ ì¡°íšŒ
        query = "SELECT stock_code, stock_name FROM stock_info WHERE industry = %s LIMIT 10"
        cursor.execute(query, (industry,))
        stocks = cursor.fetchall()
        return jsonify({'status': 'success', 'stocks': stocks})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500
    finally:
        if conn and conn.is_connected(): conn.close()

@app.route('/')
def index():
    return render_template('index.html')


@app.route('/get_simple_chart_data', methods=['POST'])
def get_simple_chart_data():
    data = request.json
    ticker = data.get('ticker')
    period = data.get('period', '1y')
    
    chart_data = get_stock_data_from_yfinance(ticker, period=period)
    
    if chart_data:
        return jsonify(chart_data)
    else:
        return jsonify({'status': 'error', 'message': f"'{ticker}' ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}), 404


@app.route('/get_current_price', methods=['POST'])
def get_current_price():
    ticker = request.json.get('ticker')
    if not ticker:
        return jsonify({'price': None, 'time': 'N/A'})

    try:
        data = yf.download(ticker, period='1d', interval='1m', progress=False, auto_adjust=True)
        if not data.empty:
            latest_price = data['Close'].iloc[-1]
            latest_time_utc = data.index[-1].tz_convert('UTC')
            kst = pytz.timezone('Asia/Seoul')
            latest_time_kst = latest_time_utc.astimezone(kst)

            # ì„±ê³µí–ˆë˜ ì½”ë“œë¡œ ë³µêµ¬
            return jsonify({'price': float(latest_price), 'time': latest_time_kst.strftime('%Y-%m-%d %H:%M:%S')})
    except Exception as e:
        print(f"í˜„ì¬ê°€ ì¡°íšŒ ì˜¤ë¥˜: {e}")

    return jsonify({'price': None, 'time': 'Error'})

@app.route('/get_stock_names', methods=['POST'])
def get_stock_names_api():
    data = request.json
    tickers = data.get('tickers', [])
    if not tickers:
        return jsonify({})
    names = get_and_cache_stock_names(tickers)
    return jsonify(names)

@app.route('/switch_ticker', methods=['POST'])
def switch_ticker():
    data = request.get_json(silent=True) or {}
    ticker = (data.get('ticker') or '').strip()
    historical_days = int(data.get('historical_days', 365))

    if not ticker:
        return jsonify({'status': 'error', 'message': 'ticker is required'}), 400

    def run_training(tk, hist_days):
        try:
            app.logger.info(f"[BG] training start: {tk}")
            train_and_predict_all_models(ticker=tk, historical_days=hist_days)
            socketio.emit('training_complete', {'status': 'success', 'ticker': tk})
        except Exception as e:
            app.logger.exception(f"[BG] training error: {e}")
            socketio.emit('training_error', {'status': 'error', 'ticker': tk, 'message': str(e)})

    socketio.start_background_task(run_training, ticker, historical_days)
    return jsonify({'status': 'ok', 'message': f"'{ticker}' ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤."}), 202


@app.route('/get_ai_results', methods=['POST'])
def get_ai_results():
    data = request.json
    ticker = data.get('ticker')
    if not ticker:
        return jsonify({'status': 'error', 'message': 'Ticker is required'}), 400
    
    print(f"--- ì›¹ ë¸Œë¼ìš°ì €ì˜ ìš”ì²­ì— ë”°ë¼ AI ì˜ˆì¸¡ ê¸°ë¡ ì¡°íšŒ: {ticker} ---")
    historical_data = get_historical_data_from_db(ticker)
    
    if historical_data and historical_data.get('dates'):
        return jsonify(historical_data)
    else:
        # ì„±ê³µí–ˆë˜ ì½”ë“œë¡œ ë³µêµ¬
        return jsonify({'status': 'error', 'message': 'DBì—ì„œ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'}), 404

@app.route('/get_recommendations', methods=['POST'])
def get_recommendations():
    """
    'AI ì˜ˆì¸¡ ìƒìŠ¹ë¥  Top 5'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ëª¨ë“  'base' ëª¨ë¸ì˜ í‰ê·  ì˜ˆì¸¡ê°€ - ìµœì‹  ì‹¤ì œ ì¢…ê°€) / ìµœì‹  ì‹¤ì œ ì¢…ê°€
    """
    
    top_movers = []
    conn = get_db_connection()
    if not conn:
        return jsonify({'status': 'error', 'message': 'DB connection failed'}), 500

    try:
        cursor = conn.cursor(dictionary=True)
        
        # AI ì˜ˆì¸¡ ìƒìŠ¹ë¥  Top 5ë¥¼ ê³„ì‚°í•˜ëŠ” SQL ì¿¼ë¦¬ (ëª¨ë¸ í‰ê·  ì‚¬ìš©)
        query = """
            WITH NextPredictions AS (
                -- 1. ê° ì¢…ëª©ë³„ 'base' ëª¨ë¸ì˜ 'í‰ê· ' ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
                SELECT
                    stock_code,
                    AVG(predicted_price) as avg_predicted_price
                FROM future_predictions
                WHERE prediction_date >= CURRENT_DATE()
                  AND variant = 'base'
                GROUP BY stock_code
            ),
            LastActualPrice AS (
                -- 2. ê° ì¢…ëª©ì˜ ê°€ì¥ ìµœì‹  ì‹¤ì œ ì¢…ê°€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                SELECT
                    stock_code,
                    close AS last_close,
                    ROW_NUMBER() OVER(PARTITION BY stock_code ORDER BY date DESC) as rn
                FROM daily_price
            ),
            PredictedMovers AS (
                -- 3. í‰ê·  ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì¢…ê°€ë¥¼ ë¹„êµí•˜ì—¬ 'í‰ê·  ì˜ˆì¸¡ ìƒìŠ¹ë¥ 'ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
                SELECT
                    p.stock_code,
                    a.last_close,
                    p.avg_predicted_price,
                    ((p.avg_predicted_price - a.last_close) / a.last_close) * 100 AS predicted_change_percent
                FROM NextPredictions p
                JOIN LastActualPrice a ON p.stock_code = a.stock_code
                WHERE a.rn = 1 AND a.last_close > 0 AND p.avg_predicted_price IS NOT NULL
            )
            -- 4. ìƒìŠ¹ë¥ ì´ 0%ë³´ë‹¤ í° ì¢…ëª©ë§Œ Top 5ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
            SELECT
                m.stock_code,
                s.stock_name,
                m.predicted_change_percent
            FROM PredictedMovers m
            LEFT JOIN stock_info s ON m.stock_code = s.stock_code
            WHERE m.predicted_change_percent > 0
            ORDER BY m.predicted_change_percent DESC
            LIMIT 5
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        for row in results:
            top_movers.append({
                'ticker': row['stock_code'],
                'name': row['stock_name'] or row['stock_code'],
                'change': f"{row['predicted_change_percent']:.2f}%"
            })
            
    except Exception as e:
        print(f"AI ë­í‚¹(ëª¨ë¸ í‰ê· ) ê¸°ë°˜ ì¶”ì²œ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        if conn and conn.is_connected(): conn.close()
    
    return jsonify({
        'top_movers': top_movers
    })

@app.route('/analyze_news_sentiment', methods=['POST'])
def analyze_news_sentiment_api():
    start_total = time.time()

    data = request.json
    ticker = data.get('ticker')
    requested_index = data.get('index', 0) 
    mode = data.get('mode', 'bert') # â˜… mode íŒŒë¼ë¯¸í„° ì¶”ê°€

    global GLOBAL_NEWS_CACHE
    if ticker not in GLOBAL_NEWS_CACHE:
        GLOBAL_NEWS_CACHE[ticker] = {}

    if mode == 'bert' and not GLOBAL_MODELS:
        return jsonify({'status': 'error', 'message': 'BERT ëª¨ë¸ ë¯¸ë¡œë“œ'}), 503
    if mode == 'llm' and GLOBAL_GEMINI_MODEL is None:
        print("ğŸš¨ LLM ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ API í‚¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        return jsonify({'status': 'error', 'message': 'Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (503)'}), 503


    # 1ï¸âƒ£ ì¢…ëª© ì´ë¦„ ì¡°íšŒ
    stock_names = get_and_cache_stock_names([ticker])
    name = stock_names.get(ticker, ticker)

    # 2ï¸âƒ£ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ë° í•„í„°ë§
    if ticker in ['AAPL', 'TSLA', 'MSFT', 'GOOGL']:
        query = ticker
    else:
        short_name = re.sub(r'\([^)]*\)|\s*ìœ ê°€ì¦ê¶Œ|ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|\s*\(KOSPI\)|\s*\(KOSDAQ\)', '', name).strip()
        query = short_name if short_name else name
    
    articles = fetch_korean_news(query=query, page_size=20) 
    
    if not articles:
        return jsonify({
            'status': 'error', 
            'message': 'News APIë¡œë¶€í„° ì¢…ëª© ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì „í˜€ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (API í‚¤/í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°€ëŠ¥ì„±)',
            'total_valid_news': 0
        }), 404


    articles.sort(key=lambda x: x.get('publishedAt', ''), reverse=True)
    seen = set()
    unique_articles = []
    for a in articles:
        if a['title'] not in seen:
            unique_articles.append(a)
            seen.add(a['title'])
    articles = unique_articles
    
    valid_count = 0
    target_raw_index = -1
    target_article = None
    
    for raw_index, article in enumerate(articles):
        title = article.get('title', '')
        key = normalize_key(title)
        
        is_valid = False
        sentences_to_analyze = []

        # ìºì‹œ í‚¤ì— modeë¥¼ í¬í•¨í•˜ì—¬ BERTì™€ LLM ê²°ê³¼ë¥¼ ë¶„ë¦¬ ì €ì¥
        cache_key_with_mode = f"{key}_{mode}"
        
        if cache_key_with_mode in GLOBAL_NEWS_CACHE[ticker]:
            is_valid = True
            
        else:
            text = article.get('content') or article.get('description') or ""
            
            if re.search(r'\[\+\d+ chars\]', text):
                 text = re.sub(r'\[\+\d+ chars\]', '', text)
            if not is_valid_sentence(text) and article.get('url'):
                text = fetch_full_article(article['url'])
            
            cleaned_text = re.sub(r'\[\+\d+ chars\]', '', text)
            
            # BERTëŠ” ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬, LLMì€ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            if mode == 'bert':
                sentences = kss.split_sentences(cleaned_text[:2000]) 
                sentences_to_analyze = [s for s in sentences if s.strip() and is_valid_sentence(s)]
                is_valid = len(sentences_to_analyze) > 0
            elif mode == 'llm':
                 # LLMì€ 100ì ë¯¸ë§Œ ê¸°ì‚¬ë§Œ ì œì™¸í•˜ê³  ìœ íš¨í•˜ë‹¤ê³  ê°„ì£¼
                 is_valid = len(cleaned_text.strip()) >= 100
            
            if not is_valid: continue 

        valid_count += 1
        
        if valid_count == requested_index + 1:
            target_raw_index = raw_index
            target_article = article
            # ìºì‹œ ë¯¸ìŠ¤ì˜€ì„ ê²½ìš°, ë¶„ì„ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ë¥¼ ê¸°ì‚¬ì— ì„ì‹œ ì €ì¥
            if not is_valid:
                target_article['temp_cleaned_text'] = cleaned_text
                target_article['temp_sentences'] = sentences_to_analyze
            break
            
    if target_raw_index == -1:
        # ìœ íš¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ íŠ¹ì • ë©”ì‹œì§€ ë°˜í™˜
        return jsonify({
            'status': 'error', 
            'message': 'í˜„ì¬ ì„ íƒëœ ì¢…ëª©ì— ëŒ€í•´ ë¶„ì„í•  ìœ íš¨í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ ê²€ìƒ‰ì–´ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.',
            'total_valid_news': valid_count
        }), 404

    article = target_article
    title = article.get('title', '')
    key = normalize_key(title)
    cache_key_with_mode = f"{key}_{mode}"
    
    if cache_key_with_mode in GLOBAL_NEWS_CACHE[ticker]:
        cached_result = GLOBAL_NEWS_CACHE[ticker][cache_key_with_mode].copy()
        cached_result['status'] = 'success (cached)'
        cached_result['analyzed_index'] = target_raw_index
        return jsonify(cached_result)

    # B. ë¶„ì„ ìˆ˜í–‰
    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if 'temp_cleaned_text' in article:
            cleaned_text = article['temp_cleaned_text']
            sentences_to_analyze = article.get('temp_sentences', [])
        else:
            text = article.get('content') or article.get('description') or ""
            if re.search(r'\[\+\d+ chars\]', text): text = re.sub(r'\[\+\d+ chars\]', '', text)
            if not is_valid_sentence(text) and article.get('url'): text = fetch_full_article(article['url'])
            cleaned_text = re.sub(r'\[\+\d+ chars\]', '', text)
            
            if mode == 'bert':
                cleaned_text = cleaned_text[:2000]
                sentences = kss.split_sentences(cleaned_text)
                sentences_to_analyze = [s for s in sentences if s.strip() and is_valid_sentence(s)]


        # 2. BERT ëª¨ë“œì™€ LLM ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬
        if mode == 'bert':
            all_results = [analyze_sentiment(s, GLOBAL_MODELS) for s in sentences_to_analyze]
            final_labels = [r['final_label'] for r in all_results if r['final_label'] != 'NEUTRAL']
            pos = final_labels.count('POSITIVE')
            neg = final_labels.count('NEGATIVE')
            overall = 'POSITIVE' if pos > neg else 'NEGATIVE' if neg > pos else 'NEUTRAL'
            avg1 = np.mean([r['model1_score'] for r in all_results]) if all_results else 0
            avg2 = np.mean([r['model2_score'] for r in all_results]) if all_results else 0

            analysis_result = {
                "final_label": overall,
                "model1_score": float(avg1),
                "model2_score": float(avg2),
                "positive_count": pos,
                "negative_count": neg,
                "analyzed_sentences": sentences_to_analyze[:5],
                "total_sentences": len(sentences_to_analyze),
                "analysis_mode": "BERT"
            }
        
        elif mode == 'llm':
            llm_response = analyze_sentiment_with_llm(ticker, name, cleaned_text)

            if llm_response.get('status') == 'error':
                return jsonify(llm_response), 500
            
            analysis_result = {
                "final_label": llm_response['final_label'],
                "summary": llm_response['summary'],
                "key_sentiment_points": llm_response['key_sentiment_points'],
                "analysis_mode": "LLM"
            }

        else:
             return jsonify({'status': 'error', 'message': 'ì˜ëª»ëœ ë¶„ì„ ëª¨ë“œì…ë‹ˆë‹¤.'}), 400
        
        # 3. ìµœì¢… ê²°ê³¼ í†µí•© ë° ìºì‹œ ì €ì¥
        result = {
            "status": "success",
            "ticker": ticker,
            "title": title,
            "source": article.get('source'),
            "url": article.get('url'),
            "full_original_text": cleaned_text,
            "analyzed_index": target_raw_index,
            **analysis_result 
        }

        GLOBAL_NEWS_CACHE[ticker][cache_key_with_mode] = result.copy()

        return jsonify(result)

    except Exception as e:
        traceback.print_exc()
        return jsonify({'status': 'error', 'message': str(e)}), 500
    
#(aiì „ë¶€ ìƒìŠ¹ ì˜ˆì¸¡)ì•Œë¦¼
from flask import request, jsonify

@app.post("/api/alerts/run")
def run_consensus_alerts():
    body = request.get_json(silent=True) or {}
    target_date  = body.get("date")
    direction    = (body.get("direction") or "up").lower()   # up|down|both
    min_conf     = body.get("min_confidence")                # None or float
    require_all  = bool(body.get("require_all", True))
    min_models   = body.get("min_models")                    # int or None
    alert_prefix = (body.get("alert_prefix") or "consensus").lower()  # "consensus"|"model"|...

    with get_conn() as conn, conn.cursor() as cur:
        # ìµœì‹  ë‚ ì§œ ê¸°ë³¸ê°’
        if not target_date:
            cur.execute("SELECT MAX(date) AS d FROM prediction_result")
            row = cur.fetchone()
            target_date = row["d"]
            if not target_date:
                return jsonify({"ok": False, "msg": "prediction_resultê°€ ë¹„ì–´ìˆìŒ"}), 400

        # min_conf í•„í„° ì¡°ê°
        conf_sql = "AND (confidence IS NULL OR confidence >= %s)" if (min_conf is not None) else ""
        conf_param = [float(min_conf)] if (min_conf is not None) else []

        # (stock_code, date)ë³„ up/down ì¹´ìš´íŠ¸
        group_sql = f"""
            SELECT
                stock_code,
                date,
                COUNT(*) AS n_models,
                SUM(CASE WHEN UPPER(predict_class) IN ('UP','RISE','BUY','BULL') {conf_sql} THEN 1 ELSE 0 END) AS n_up,
                SUM(CASE WHEN UPPER(predict_class) IN ('DOWN','SELL','BEAR') {conf_sql} THEN 1 ELSE 0 END)      AS n_down
            FROM prediction_result
            WHERE date = %s
            GROUP BY stock_code, date
        """

        # í•©ì˜ ì¡°ê±´ì‹
        up_cond_sql = "g.n_up = g.n_models" if require_all else "g.n_up >= COALESCE(%s, CEIL(g.n_models/2))"
        dn_cond_sql = "g.n_down = g.n_models" if require_all else "g.n_down >= COALESCE(%s, CEIL(g.n_models/2))"

        inserted_total = 0

        def insert_by_direction(is_up: bool):
            nonlocal inserted_total
            alert_type   = f"{alert_prefix}_{'up' if is_up else 'down'}"
            dir_label    = "ìƒìŠ¹" if is_up else "í•˜ë½"
            consensus_tag = " í•©ì˜" if alert_prefix == "consensus" else ""
            conf_tag     = f" (confâ‰¥{min_conf})" if min_conf is not None else ""
            models_tag   = ""
            if not require_all:
                if min_models is not None:
                    models_tag = f" (â‰¥{int(min_models)} models)"
                else:
                    models_tag = " (â‰¥ê³¼ë°˜)"  # ê³¼ë°˜ ê¸°ë³¸
            # ë©”ì‹œì§€ ì•ë¶€ë¶„ì€ íŒŒì´ì¬ì—ì„œ ë§Œë“¤ê³  SQLì—ì„œëŠ” CONCAT(%s, GROUP_CONCAT(...))
            message_prefix = f"ëª¨ë¸{consensus_tag} {dir_label}{conf_tag}{models_tag} | ì°¸ì—¬ëª¨ë¸: "

            cond_sql = up_cond_sql if is_up else dn_cond_sql

            sql = f"""
                INSERT INTO user_alerts (stock_code, date, alert_type, message)
                SELECT
                  pr.stock_code,
                  pr.date,
                  %s AS alert_type,
                  CONCAT(%s, GROUP_CONCAT(DISTINCT pr.model_name ORDER BY pr.model_name SEPARATOR ', ')) AS message
                FROM prediction_result pr
                JOIN (
                    {group_sql}
                ) g
                  ON g.stock_code = pr.stock_code AND g.date = pr.date
                LEFT JOIN user_alerts ua
                  ON ua.stock_code = pr.stock_code
                 AND ua.date = pr.date
                 AND ua.alert_type = %s
                WHERE pr.date = %s
                  AND {cond_sql}
                  AND ua.id IS NULL
                GROUP BY pr.stock_code, pr.date
            """

            params = [
                alert_type,            # alert_type
                message_prefix,        # message prefix
            ]
            # group_sql íŒŒë¼ë¯¸í„°ë“¤
            params += conf_param + conf_param + [target_date]
            # LEFT JOIN alert_type, pr.date
            params += [alert_type, target_date]
            # require_all=Falseë©´ min_models ë°”ì¸ë”©
            if not require_all:
                params += [min_models]
            cur.execute(sql, params)
            inserted_total += cur.rowcount

        # ì‹¤í–‰
        if direction in ("up", "both"):
            insert_by_direction(is_up=True)
        if direction in ("down", "both"):
            insert_by_direction(is_up=False)

        return jsonify({
            "ok": True,
            "date": str(target_date),
            "direction": direction,
            "require_all": require_all,
            "min_models": min_models,
            "min_confidence": min_conf,
            "inserted": inserted_total
        })

#ì•Œë¦¼ ì¡°íšŒ ë¼ìš°íŠ¸
@app.get("/api/alerts")
def list_alerts():
    stock_code = request.args.get("stock_code")
    date_from  = request.args.get("date_from")
    date_to    = request.args.get("date_to")
    a_type     = request.args.get("type")  # ì˜ˆ: consensus_up, consensus_down, model_up, model_down

    clauses, params = [], []
    if a_type:
        clauses.append("alert_type = %s"); params.append(a_type)
    if stock_code:
        clauses.append("stock_code = %s"); params.append(stock_code)
    if date_from:
        clauses.append("date >= %s"); params.append(date_from)
    if date_to:
        clauses.append("date <= %s"); params.append(date_to)

    where_sql = ("WHERE " + " AND ".join(clauses)) if clauses else ""
    q = f"""
      SELECT id, stock_code, date, alert_type, message, created_at
      FROM user_alerts
      {where_sql}
      ORDER BY date DESC, stock_code
      LIMIT 500
    """
    with get_conn() as conn, conn.cursor() as cur:
        cur.execute(q, params)
        rows = cur.fetchall()
    return jsonify({"ok": True, "count": len(rows), "items": rows})

# ==============================================================================
# 7. ì„œë²„ ì´ˆê¸°í™”
# ==============================================================================
def initialize():
    load_models() # BERT ë¡œë“œ
    initialize_gemini() # Gemini ë¡œë“œ
    initialize_stock_data()
    print("âœ… ëª¨ë“  ì´ˆê¸°í™” ì™„ë£Œ.")

if __name__ == '__main__':
    initialize()
    print("--- Eventlet ê¸°ë°˜ ê³ ì„±ëŠ¥ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. http://127.0.0.1:5000 ---")
    socketio.run(app, host='0.0.0.0', port=5000)